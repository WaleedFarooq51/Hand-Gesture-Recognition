# Human Hand Gesture Recognition (Sign Language Detection)

This repository implements a **real-time sign-language recognition system** using **TensorFlow Object Detection API**. The project captures hand gestures (including the American Sign Language alphabet) via a webcam (OpenCV), detects and localizes hands, and maps detected gestures to alphabet letters or common gestures. 

---

## ğŸ” Key Highlights

- Real-time detection via webcam using OpenCV.
- Transfer learning from a COCO-pretrained MobileNet SSD (V2) backbone.
- Training and inference using TensorFlow Object Detection API.
  
---

## ğŸ“ Repository Structure

```
.
â”œâ”€ workspace/
â”‚  â”œâ”€ images/
â”‚  â”‚  â”œâ”€ train/                # training images
â”‚  â”‚  â””â”€ test/                 # testing images
â”‚  â”œâ”€ annotations/            
â”‚  â”‚  â”œâ”€ label_map.pbtxt       # LabelMe JSON / XML annotations
â”‚  â”‚  â”œâ”€ train.record          # TFRecord for training
â”‚  â”‚  â””â”€ test.record           # TFRecord for evaluation
â”‚  â”œâ”€ models/                  # trained model checkpoints
â”‚  â”‚  â”œâ”€ my_ssd_mobnet
â”‚  â”œâ”€ pre-trained models       # pre-trained SSD-MobNet model checkpoints    
â”œâ”€ scripts/
â”‚  â”œâ”€ generate_tfrecord.py     # convert annotations -> TFRecord
â”œâ”€ README.md
â””â”€ requirements.txt
```

---

## ğŸ§© Dataset & Annotation

- The dataset contains images for **26 alphabet signs (Aâ€“Z)** and additional custom gestures. Some data were pre-annotated; other images were captured with a webcam and annotated manually using **LabelMe**.
- Annotations include bounding boxes (coordinates) and class labels.
- A `label_map.pbtxt` file maps numeric class IDs to human-readable labels.
  
---

## ğŸ›  Tools & Frameworks

- Python 3.8+
- TensorFlow 2.x (Object Detection API v2)
- OpenCV (cv2) for webcam capture and display
- LabelMe for annotation (or any bounding-box annotation tool)
- `protoc` for compiling TF record utilities

---

## ğŸ”§ Model architecture

MobileNetV2 + SSD (pretrained on COCO 2017, input 320Ã—320). Transfer learning fine-tunes the detection head on the custom hand-gesture dataset.

---

## âœ… Results & Evaluation

- Two types of inference were run: (1) image-based testing on held-out images, and (2) **real-time webcam inference**.

- The model returns bounding boxes with class labels and confidence scores.

![Sample Detection](./results/img1.png) ![Sample Detection](./results/img2.png) ![Sample Detection](./results/img3.png)

**Limitations observed:**
- Uncontrolled backgrounds and low-light conditions reduce accuracy.
- Varied skin tones, clothing, partial occlusions, and face presence can confuse the detector.
- Some similar letters or gestures are confused with each other â€” remedyable with more data and augmentation. îˆ€fileciteîˆ‚turn1file0îˆ

---

## ğŸš€ Quickstart 

### 1) Clone repo
```bash
git clone <your-repo-url>
cd <repo-name>
```

Open `detection.ipynb' notebook.

### 2) Prepare dataset & label map
- Put images into `workspace/images/train` and `workspace/images/test`.
- Create a `label_map.pbtxt`.
- Ensure your `label_map.pbtxt` lists all classes (Aâ€“Z and custom gestures).

### 3) Install required libraries & TF Object detection API

### 4) Generate TFRecords
```bash
python scripts/generate_tfrecord.py -x "path to train images -l "path to label_map.pbtxt -o "path to store tf_records"
python scripts/generate_tfrecord.py -x "path to test images -l "path to label_map.pbtxt -o "path to store tf_records"
```

### 4) Training the model
```bash
python model_main_tf2.py --model_dir=models/my_ssd_mobnet --pipeline_config_path=models/my_ssd_mobnet/pipeline.config
```
---


